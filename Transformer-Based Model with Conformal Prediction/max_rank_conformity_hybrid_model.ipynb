{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T13:47:38.417472Z",
     "iopub.status.busy": "2025-03-07T13:47:38.417119Z",
     "iopub.status.idle": "2025-03-07T13:47:38.423160Z",
     "shell.execute_reply": "2025-03-07T13:47:38.422194Z",
     "shell.execute_reply.started": "2025-03-07T13:47:38.417440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import pandas as pd  # type: ignore\n",
    "import numpy as np\n",
    "\n",
    "from conformal_hybrid import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "save_dir = Path(\"models_hybrid\")\n",
    "\n",
    "all_data = load_and_preprocess_data_hybrid()\n",
    "pickle.dump(all_data, open(Path(save_dir, f\"all_data.pkl\"), \"wb\"))\n",
    "\n",
    "dropout_rate = 0.25\n",
    "hidden_dimension_1 = 2048\n",
    "hidden_dimension_2 = 128\n",
    "weight_decay = 0.1\n",
    "lr = 4e-5\n",
    "\n",
    "if RETRAIN:\n",
    "    for i in range(ENSEMBLE_MODELS):\n",
    "        # for i in [0]:\n",
    "        print(\"##############################################################\")\n",
    "        print(f\"Training model {i}\")\n",
    "        print(f\"Loading and preprocessing data for model {i}\")\n",
    "\n",
    "        data = all_data[i]\n",
    "        save_path = Path(save_dir, f\"submission_{i}.pth\")\n",
    "\n",
    "        # model, tokenizer, best_threshold = train_model(data, num_epochs=3)\n",
    "        # model, tokenizer, best_threshold, metrics_df = train_model(data, num_epochs=3)\n",
    "        model, tokenizer, best_threshold, epoch_metrics, step_metrics = (\n",
    "            train_model_hybrid(\n",
    "                data,\n",
    "                num_epochs=EPOCHS,\n",
    "                learning_rate=lr,\n",
    "                log_steps=25,\n",
    "                name=f\"model_{i}\",\n",
    "                save_dir=save_dir,\n",
    "                hidden_dimension_1=hidden_dimension_1,\n",
    "                hidden_dimension_2=hidden_dimension_2,\n",
    "                dropout_rate=dropout_rate,\n",
    "                weight_decay=weight_decay,\n",
    "            )\n",
    "        )\n",
    "        model.model_summary()\n",
    "        if not Path(save_dir).exists():\n",
    "            Path(save_dir).mkdir()\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        # pickle.dump(data, open(Path(save_dir, f\"data_{i}.pkl\"), \"wb\"))\n",
    "        del model\n",
    "        print(f\"Done training model {i}\")\n",
    "        print(\n",
    "            \"##############################################################\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "models = []\n",
    "# Load the trained model\n",
    "for i in range(ENSEMBLE_MODELS):\n",
    "    model = GenreClassifierHybrid(\n",
    "        n_genres=20,\n",
    "        hidden_dimension_1=hidden_dimension_1,\n",
    "        hidden_dimension_2=hidden_dimension_2,\n",
    "        dropout_rate=dropout_rate,\n",
    "    )\n",
    "    save_path = Path(save_dir, f\"submission_{i}.pth\")\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    # data = pickle.load(open(Path(save_dir, f\"data_{i}.pkl\"), \"rb\"))\n",
    "    models.append({\"model\": model, \"save_path\": save_path})\n",
    "    print(f\"Loaded model {i}\")\n",
    "\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "best_threshold = 0.5\n",
    "print(\"Computing calibration scores for conformal prediction...\")\n",
    "\n",
    "submissions = []\n",
    "i = 0\n",
    "# for i in range(N_MODELS):\n",
    "data = all_data[i]\n",
    "model = models[i][\"model\"]\n",
    "\n",
    "indices = np.arange(len(data[\"val_texts\"]))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "val_texts = data[\"val_texts\"][train_idx]\n",
    "val_numeric = data[\"val_numeric\"][train_idx]\n",
    "val_labels = data[\"val_labels\"][train_idx]\n",
    "eval_texts = data[\"val_texts\"][test_idx]\n",
    "eval_numeric = data[\"val_numeric\"][test_idx]\n",
    "eval_labels = data[\"val_labels\"][test_idx]\n",
    "\n",
    "# Update data dictionary for calibration\n",
    "cal_data = {\n",
    "    \"train_texts\": None,\n",
    "    \"val_texts\": val_texts,\n",
    "    \"train_numeric\": data[\"train_numeric\"],\n",
    "    \"val_numeric\": val_numeric,\n",
    "    \"train_labels\": None,\n",
    "    \"val_labels\": val_labels,\n",
    "    \"test_texts\": eval_texts,\n",
    "    \"test_numeric\": eval_numeric,\n",
    "    \"test_ids\": np.arange(len(eval_texts)),\n",
    "    \"genre_columns\": data[\"genre_columns\"],\n",
    "}\n",
    "\n",
    "best_alpha_max, best_q_max, best_metric_max = find_optimal_alpha_hybrid(\n",
    "    model,\n",
    "    data,\n",
    "    tokenizer,\n",
    "    # nonconformity_scores,\n",
    "    best_threshold,\n",
    "    alpha_values=np.linspace(0.025, 0.2, 20),\n",
    "    conformal_scores_function=compute_calibration_scores_hybrid_max,\n",
    "    prediction_function=predict_with_conformal_hybrid_max,\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "best_alpha, best_q, best_metric = find_optimal_alpha_hybrid(\n",
    "    model,\n",
    "    data,\n",
    "    tokenizer,\n",
    "    # nonconformity_scores,\n",
    "    best_threshold,\n",
    "    alpha_values=np.linspace(0.025, 0.2, 20),\n",
    "    conformal_scores_function=compute_calibration_scores_hybrid,\n",
    "    prediction_function=predict_with_conformal_hybrid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(\n",
    "    f\"Best alpha for hybrid max: {best_alpha_max}, q: {best_q_max}, metric: {best_metric_max}\"\n",
    ")\n",
    "print(\n",
    "    f\"Best alpha for hybrid: {best_alpha}, q: {best_q}, metric: {best_metric}\"\n",
    ")\n",
    "\n",
    "cal_scores_max = compute_calibration_scores_hybrid_max(\n",
    "    model, cal_data, tokenizer\n",
    ")\n",
    "\n",
    "submission_max, probs, q, standard_preds = predict_with_conformal_hybrid_max(\n",
    "    model, data, tokenizer, cal_scores_max, threshold=0.5, alpha=best_alpha_max\n",
    ")\n",
    "\n",
    "\n",
    "cal_scores = compute_calibration_scores_hybrid(model, cal_data, tokenizer)\n",
    "\n",
    "submission, probs, q, standard_preds = predict_with_conformal_hybrid(\n",
    "    model, data, tokenizer, cal_scores, threshold=0.5, alpha=best_alpha\n",
    ")\n",
    "\n",
    "print(\"Saving submission file...\")\n",
    "submission.to_csv(Path(save_dir, \"submission.csv\"), index=False)\n",
    "submission_max.to_csv(Path(save_dir, \"submission_max.csv\"), index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df_sample = pd.read_csv(data_dir + \"/sample.csv\")\n",
    "\n",
    "genre_to_idx = {genre: idx for idx, genre in enumerate(df_sample.columns[1:])}\n",
    "idx_to_genre = {idx: genre for genre, idx in genre_to_idx.items()}\n",
    "# Ensure your model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "data[\"test_texts\"][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "train_idx = 9\n",
    "\n",
    "val_text = data[\"test_texts\"][train_idx]\n",
    "val_numeric = data[\"test_numeric\"][train_idx]\n",
    "\n",
    "# val_text = \"* Title: 2001: A Space Odyssey [SEP]* Overview: visionary epic chronicling humanity's evolution and its transformative encounter with mysterious alien intelligence, triggered by the discovery of an enigmatic monolith.\"\n",
    "val_text = val_text[:216]\n",
    "print(val_text)\n",
    "\n",
    "# Tokenize input\n",
    "encoding = tokenizer(val_text, return_tensors=\"pt\").to(device)\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "print(len(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "# First, get embeddings layer explicitly\n",
    "embedding_layer = model.bert.embeddings.word_embeddings\n",
    "\n",
    "# Generate embeddings as leaf tensors directly from embedding_layer\n",
    "embeddings = embedding_layer(input_ids)\n",
    "embeddings.retain_grad()  # Explicitly retain grad if embeddings is non-leaf\n",
    "embeddings.requires_grad_()  # Ensure requires_grad is True\n",
    "\n",
    "# Forward pass manually (since we have custom embeddings)\n",
    "outputs = model.bert(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "\n",
    "cls_output = outputs.last_hidden_state[:, 0, :]  # shape: [1, hidden_dim]\n",
    "numeric_tensor = (\n",
    "    torch.tensor(val_numeric).to(device).unsqueeze(0)\n",
    ")  # [1, num_features]\n",
    "\n",
    "hybrid_input = torch.cat(\n",
    "    [cls_output, numeric_tensor], dim=1\n",
    ")  # [1, hidden_dim + num_features]\n",
    "hybrid_input = hybrid_input.to(torch.float)\n",
    "\n",
    "\n",
    "# Forward through classifier\n",
    "logits = model.classifier(hybrid_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "category1 = \"Comedy\"\n",
    "category2 = \"Science.Fiction\"\n",
    "\n",
    "target_class = genre_to_idx[category1]\n",
    "score = logits[:, target_class]\n",
    "\n",
    "# Compute gradients w.r.t. embeddings\n",
    "model.zero_grad()\n",
    "score.backward(retain_graph=True)\n",
    "\n",
    "# Extract gradients\n",
    "saliency_gradients = embeddings.grad.data.abs().squeeze(\n",
    "    0\n",
    ")  # [seq_len, emb_size]\n",
    "\n",
    "token_gradients_1 = saliency_gradients.mean(dim=1)  # [seq_len]\n",
    "# token_gradients_comedy /= (\n",
    "#     token_gradients_comedy.max()\n",
    "# )  # Normalize gradients between [0,1]\n",
    "\n",
    "\n",
    "# Choose a target class to compute saliency (e.g., first class, index 0)\n",
    "target_class = genre_to_idx[category2]\n",
    "score = logits[:, target_class]\n",
    "\n",
    "# Compute gradients w.r.t. embeddings\n",
    "model.zero_grad()\n",
    "score.backward(retain_graph=True)\n",
    "\n",
    "# Extract gradients\n",
    "saliency_gradients = embeddings.grad.data.abs().squeeze(\n",
    "    0\n",
    ")  # [seq_len, emb_size]\n",
    "\n",
    "token_gradients_2 = saliency_gradients.mean(dim=1)  # [seq_len]\n",
    "# token_gradients_2 /= (\n",
    "#     token_gradients_2.max()\n",
    "# )  # Normalize gradients between [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_gradients = token_gradients_2 - token_gradients_1\n",
    "token_gradients /= token_gradients.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "token_gradients_np = token_gradients.cpu().numpy()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "plt.figure(figsize=(len(tokens) * 0.5, 2))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Create color-coded heatmap\n",
    "gradient_array = np.expand_dims(\n",
    "    token_gradients_np, axis=0\n",
    ")  # shape [1, seq_len]\n",
    "\n",
    "# Display gradient as heatmap\n",
    "cax = ax.matshow(gradient_array, cmap=\"Reds\", aspect=\"auto\")\n",
    "\n",
    "# Set token labels on x-axis\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_xticklabels(tokens, rotation=90, fontsize=12)\n",
    "\n",
    "# Remove y-axis ticks\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Add colorbar for reference\n",
    "plt.colorbar(cax, orientation=\"vertical\", pad=0.02, fraction=0.025)\n",
    "\n",
    "plt.title(\"Saliency map\", fontsize=14, pad=20)\n",
    "plt.savefig(\"saliency_space_odyssey.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dummy data for demonstration; replace with your actual tokens and gradients.\n",
    "# tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "# token_gradients_np = token_gradients.cpu().numpy()\n",
    "\n",
    "\n",
    "# --- Step 1. Wrap tokens into lines based on a character limit per line ---\n",
    "char_limit = (\n",
    "    100  # Maximum characters per line (approximate, using token text lengths)\n",
    ")\n",
    "lines = []\n",
    "current_line = []\n",
    "current_length = 0\n",
    "\n",
    "for token, grad in zip(tokens, token_gradients_np):\n",
    "    # We add an extra space after each token.\n",
    "    token_str = token + \" \"\n",
    "    token_length = len(token_str)\n",
    "    # If adding this token exceeds the char limit and there is already content in the current line,\n",
    "    # wrap to a new line.\n",
    "    if current_length + token_length > char_limit and current_line:\n",
    "        lines.append(current_line)\n",
    "        current_line = [(token, grad)]\n",
    "        current_length = token_length\n",
    "    else:\n",
    "        current_line.append((token, grad))\n",
    "        current_length += token_length\n",
    "if current_line:\n",
    "    lines.append(current_line)\n",
    "\n",
    "# --- Step 2. Plot tokens line by line with gradient-coded backgrounds ---\n",
    "cmap = plt.cm.Reds  # Colormap mapping gradients [0,1] to a red-scale color\n",
    "\n",
    "# Create a figure with height proportional to the number of lines.\n",
    "line_count = len(lines)\n",
    "fig, ax = plt.subplots(figsize=(10, line_count * 0.7))\n",
    "ax.axis(\"off\")  # Hide axes\n",
    "\n",
    "x_margin = 0.05  # starting x in data coordinates\n",
    "y_start = 0.9  # starting y (top of the figure)\n",
    "y_gap = 0.15  # vertical gap between lines\n",
    "\n",
    "# Get a renderer for text measurements.\n",
    "fig.canvas.draw()  # Ensure the renderer is initialized\n",
    "renderer = fig.canvas.get_renderer()\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    x_pos = x_margin\n",
    "    y_pos = y_start - i * y_gap\n",
    "    for token, grad in line:\n",
    "        color = cmap(grad)  # Map gradient to a color\n",
    "        # Draw token with a background box\n",
    "        text_obj = ax.text(\n",
    "            x_pos,\n",
    "            y_pos,\n",
    "            token + \" \",\n",
    "            fontsize=12,\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "            bbox=dict(facecolor=color, edgecolor=\"none\", pad=2),\n",
    "        )\n",
    "        # Force a draw to compute text dimensions\n",
    "        fig.canvas.draw()\n",
    "        extent = text_obj.get_window_extent(renderer=renderer)\n",
    "        # Convert extent width from display (pixel) coordinates to data coordinates\n",
    "        inv = ax.transData.inverted()\n",
    "        (x0, y0), (x1, y1) = inv.transform(\n",
    "            [(extent.x0, extent.y0), (extent.x1, extent.y1)]\n",
    "        )\n",
    "        token_width = x1 - x0\n",
    "        x_pos += token_width\n",
    "\n",
    "plt.title(\n",
    "    f\"({category2} gradient - {category1} gradient)\\n '2001: A Space Odyssey' (title and overview only)\",\n",
    "    fontsize=14,\n",
    "    pad=20,\n",
    ")\n",
    "plt.savefig(\n",
    "    Path(save_dir, \"saliency_space_odyssey_wrapped.pdf\"), bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "perplexity = 150\n",
    "n_movies = 250\n",
    "proba = 0.7\n",
    "\n",
    "# Select a subset of test sentences for visualization\n",
    "sample_texts = data[\"test_texts\"][:n_movies]\n",
    "features = []\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "for text in sample_texts:\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**encoding)\n",
    "        cls_feature = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        features.append(cls_feature[0])\n",
    "\n",
    "features = np.array(features)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "tsne_results = tsne.fit_transform(features)\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.scatter(tsne_results[:, 0], tsne_results[:, 1])\n",
    "# plt.title(\"t-SNE Visualization of Model's CLS Features\")\n",
    "# plt.show()\n",
    "\n",
    "import re\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.manifold import TSNE\n",
    "# import torch\n",
    "\n",
    "# Assuming sample_texts and tsne_results are already defined as in your notebook:\n",
    "# sample_texts = data[\"test_texts\"][:50]\n",
    "# tsne_results = tsne.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "ax = plt.gca()\n",
    "ax.scatter(tsne_results[:, 0], tsne_results[:, 1], c=\"darkred\", alpha=0.25)\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Model's CLS Features\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Loop over each sample point and annotate with the extracted movie title.\n",
    "for i, text in enumerate(sample_texts):\n",
    "    if np.random.uniform(0, 1) < proba:\n",
    "        continue\n",
    "    # Use a regular expression to find the movie title segment.\n",
    "    # This regex looks for \"* Title:\" followed by any characters until the first occurrence of \"[SEP]\"\n",
    "    match = re.search(r\"\\* Title:\\s*([^[]+)\\[SEP\\]\", text)\n",
    "    if match:\n",
    "        title = match.group(1).strip()\n",
    "    else:\n",
    "        title = \"Unknown\"\n",
    "    # Annotate the point with the movie title.\n",
    "    ax.annotate(\n",
    "        title, (tsne_results[i, 0], tsne_results[i, 1]), fontsize=8, alpha=0.7\n",
    "    )\n",
    "plt.savefig(\n",
    "    Path(save_dir, f\"tsne_{perplexity}_p_{proba}.pdf\"), bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import shap\n",
    "import torch\n",
    "\n",
    "# Initialize JavaScript in notebook mode for SHAP plots.\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "def prediction_fn(texts):\n",
    "    # Ensure texts is a list of strings\n",
    "    if not isinstance(texts, list):\n",
    "        texts = list(texts)\n",
    "    # Tokenize the list of input texts\n",
    "    encoding = tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**encoding)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        # Create dummy numerical features (zeros) with the proper shape.\n",
    "        # Assuming the hybrid model expects 4 numerical features.\n",
    "        dummy_numeric = torch.zeros(cls_embeddings.size(0), 4).to(device)\n",
    "        # Concatenate text embeddings and dummy numerical features.\n",
    "        combined = torch.cat([cls_embeddings, dummy_numeric], dim=1)\n",
    "        logits = model.classifier(combined)\n",
    "        probs = torch.sigmoid(logits)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "\n",
    "explainer = shap.Explainer(\n",
    "    prediction_fn,\n",
    "    masker=shap.maskers.Text(tokenizer),\n",
    "    output_names=[\"Genre Probabilities\"],\n",
    ")\n",
    "sample_texts_explainer = list(sample_texts[:5])\n",
    "\n",
    "shap_values = explainer(sample_texts_explainer)\n",
    "shap.plots.text(shap_values[0])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6804012,
     "sourceId": 10940715,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 259925,
     "modelInstanceId": 238254,
     "sourceId": 278168,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
